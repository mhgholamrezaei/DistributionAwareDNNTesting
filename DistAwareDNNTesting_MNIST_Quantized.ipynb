{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktxk0x7npIkm"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8jzwTI_BUqq"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "# !pip install keras\n",
        "!pip list | grep \"tensorflow\"\n",
        "!pip list | grep \"keras\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BvMLvSODgrM"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y keras tensorflow\n",
        "# !pip install -r requirements.txt\n",
        "# !pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CfdqmNMqRwep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabd182d-d662-4ff6-c7b7-3763a150f444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-metrics\n",
            "  Downloading keras_metrics-1.1.0-py2.py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from keras-metrics) (2.15.0)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ICDOU3o1WXKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19884090-0946-4c8d-aeee-169c02d418ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-model-optimization==0.7.3\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization==0.7.3) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization==0.7.3) (1.25.2)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization==0.7.3) (1.16.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.7.3\n"
          ]
        }
      ],
      "source": [
        "# !python3 -m pip install --upgrade keras==2.8.0rc0 tensorflow==2.8.2  tensorflow-model-optimization==0.7.3\n",
        "!python3 -m pip install tensorflow-model-optimization==0.7.3\n",
        "# !python3 -m pip install -q tensorflow-model-optimization==0.8.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dCrtEPeFt-E"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9POVPlijFvP0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from keras.layers import Input\n",
        "# import keras\n",
        "# import tf_keras as keras\n",
        "# from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "import keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbC2VGWnpIGn"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3uLM2mDpQUm"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sg4gLYgqo3l-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e165367-b232-4f3b-eb6a-c2a6ccf0c1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1688/1688 [==============================] - 44s 25ms/step - loss: 0.3152 - accuracy: 0.9069 - val_loss: 0.1185 - val_accuracy: 0.9657\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 25s 15ms/step - loss: 0.1194 - accuracy: 0.9634 - val_loss: 0.0919 - val_accuracy: 0.9728\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 23s 13ms/step - loss: 0.0917 - accuracy: 0.9719 - val_loss: 0.0740 - val_accuracy: 0.9777\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 24s 14ms/step - loss: 0.0765 - accuracy: 0.9758 - val_loss: 0.0635 - val_accuracy: 0.9813\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 26s 15ms/step - loss: 0.0672 - accuracy: 0.9785 - val_loss: 0.0616 - val_accuracy: 0.9817\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 22s 13ms/step - loss: 0.0594 - accuracy: 0.9817 - val_loss: 0.0646 - val_accuracy: 0.9810\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 23s 14ms/step - loss: 0.0546 - accuracy: 0.9827 - val_loss: 0.0561 - val_accuracy: 0.9843\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 25s 15ms/step - loss: 0.0503 - accuracy: 0.9841 - val_loss: 0.0554 - val_accuracy: 0.9847\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 23s 14ms/step - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.0534 - val_accuracy: 0.9857\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 22s 13ms/step - loss: 0.0440 - accuracy: 0.9867 - val_loss: 0.0520 - val_accuracy: 0.9867\n",
            "2/2 [==============================] - 2s 433ms/step - loss: 0.0475 - accuracy: 0.9911 - val_loss: 0.1176 - val_accuracy: 0.9800\n",
            "\n",
            "\n",
            "Overall Test score: 0.04446705803275108\n",
            "Overall Test accuracy: 0.9872000217437744\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 24s 14ms/step - loss: 0.3477 - accuracy: 0.8945 - val_loss: 0.1161 - val_accuracy: 0.9683\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 25s 15ms/step - loss: 0.1202 - accuracy: 0.9638 - val_loss: 0.0806 - val_accuracy: 0.9777\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 22s 13ms/step - loss: 0.0885 - accuracy: 0.9738 - val_loss: 0.0701 - val_accuracy: 0.9808\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 23s 14ms/step - loss: 0.0743 - accuracy: 0.9780 - val_loss: 0.0621 - val_accuracy: 0.9830\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 23s 14ms/step - loss: 0.0648 - accuracy: 0.9801 - val_loss: 0.0551 - val_accuracy: 0.9842\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 22s 13ms/step - loss: 0.0590 - accuracy: 0.9820 - val_loss: 0.0566 - val_accuracy: 0.9830\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 23s 14ms/step - loss: 0.0545 - accuracy: 0.9832 - val_loss: 0.0519 - val_accuracy: 0.9847\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 23s 14ms/step - loss: 0.0517 - accuracy: 0.9844 - val_loss: 0.0507 - val_accuracy: 0.9855\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 22s 13ms/step - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.0535 - val_accuracy: 0.9843\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 25s 15ms/step - loss: 0.0457 - accuracy: 0.9857 - val_loss: 0.0512 - val_accuracy: 0.9862\n",
            "\n",
            "\n",
            "Overall Test score: 0.04286748915910721\n",
            "Overall Test accuracy: 0.9857000112533569\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "\n",
        "\n",
        "def QModel1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "        image_size = 28\n",
        "\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "        x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "        x_train = x_train.astype('float32') / 255\n",
        "        x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "        # network parameters\n",
        "        input_shape = (image_size, image_size, 1)\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    elif input_tensor is None:\n",
        "        print('You have to provide input_tensor when testing.')\n",
        "        exit()\n",
        "\n",
        "    # Define the model architecture.\n",
        "    model = keras.Sequential([\n",
        "      keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "      keras.layers.Conv2D(filters=4, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Conv2D(filters=12, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(nb_classes, name='before_softmax'),\n",
        "      keras.layers.Activation('softmax')\n",
        "    ])\n",
        "\n",
        "    if train:\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # compiling\n",
        "        model.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          epochs=nb_epoch,\n",
        "          validation_split=0.1,\n",
        "        )\n",
        "\n",
        "        model = tfmot.quantization.keras.quantize_model(model)\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        train_images_subset = x_train[0:1000] # out of 60000\n",
        "        train_labels_subset = y_train[0:1000]\n",
        "\n",
        "\n",
        "        model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=500, epochs=1, validation_split=0.1)\n",
        "\n",
        "\n",
        "\n",
        "        # save model\n",
        "        model.save_weights('./QModel1.weights.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "\n",
        "        quantize_model = tfmot.quantization.keras.quantize_model\n",
        "        model = quantize_model(model)\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        model.load_weights('./QModel1.weights.h5')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    qmodel1 = QModel1(train=True)\n",
        "\n",
        "\n",
        "\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "        image_size = 28\n",
        "\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "        x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "        x_train = x_train.astype('float32') / 255\n",
        "        x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "        # network parameters\n",
        "        input_shape = (image_size, image_size, 1)\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    elif input_tensor is None:\n",
        "        print('You have to provide input_tensor when testing.')\n",
        "        exit()\n",
        "\n",
        "    # Define the model architecture.\n",
        "    model = keras.Sequential([\n",
        "      keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "      keras.layers.Conv2D(filters=4, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Conv2D(filters=12, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(nb_classes, name='before_softmax'),\n",
        "      keras.layers.Activation('softmax')\n",
        "    ])\n",
        "\n",
        "    if train:\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # compiling\n",
        "        model.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          epochs=nb_epoch,\n",
        "          validation_split=0.1,\n",
        "        )\n",
        "\n",
        "\n",
        "        # save model\n",
        "        model.save_weights('./Model1.weights.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.load_weights('./Model1.weights.h5')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdcZJhWTpGjI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5_DWQGpTz3"
      },
      "source": [
        "## Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pacEC1FpWA8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "\n",
        "\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "        image_size = 28\n",
        "\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "        x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "        x_train = x_train.astype('float32') / 255\n",
        "        x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "        # network parameters\n",
        "        input_shape = (image_size, image_size, 1)\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    elif input_tensor is None:\n",
        "        print('You have to provide input_tensor when testing.')\n",
        "        exit()\n",
        "\n",
        "    # Define the model architecture.\n",
        "    model = keras.Sequential([\n",
        "      keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "      keras.layers.Conv2D(filters=6, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Conv2D(filters=16, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(nb_classes, name='before_softmax'),\n",
        "      keras.layers.Activation('softmax')\n",
        "    ])\n",
        "\n",
        "    if train:\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # compiling\n",
        "        model.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          epochs=nb_epoch,\n",
        "          validation_split=0.1,\n",
        "        )\n",
        "\n",
        "\n",
        "        # save model\n",
        "        model.save_weights('./Model2.weights.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.load_weights('./Model2.weights.h5')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMJfb2b3pc_M"
      },
      "source": [
        "## Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjyGs9Ubpfvx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "\n",
        "\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "        image_size = 28\n",
        "\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "        x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "        x_train = x_train.astype('float32') / 255\n",
        "        x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "        # network parameters\n",
        "        input_shape = (image_size, image_size, 1)\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    elif input_tensor is None:\n",
        "        print('You have to provide input_tensor when testing.')\n",
        "        exit()\n",
        "\n",
        "    # Define the model architecture.\n",
        "    model = keras.Sequential([\n",
        "      keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "      keras.layers.Conv2D(filters=6, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Conv2D(filters=16, kernel_size=kernel_size, activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(120, activation='relu'),\n",
        "      keras.layers.Dense(84, activation='relu'),\n",
        "      keras.layers.Dense(nb_classes, name='before_softmax'),\n",
        "      keras.layers.Activation('softmax')\n",
        "    ])\n",
        "\n",
        "    if train:\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # compiling\n",
        "        model.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          epochs=nb_epoch,\n",
        "          validation_split=0.1,\n",
        "        )\n",
        "\n",
        "\n",
        "        # save model\n",
        "        model.save_weights('./Model3.weights.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.load_weights('./Model3.weights.h5')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6105NVJUpkDg"
      },
      "source": [
        "## Model 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DhFcfgnfplkO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Model4(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "        image_size = 28\n",
        "\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        mnist = tf.keras.datasets.mnist\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "        x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "        x_train = x_train.astype('float32') / 255\n",
        "        x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "        # network parameters\n",
        "        input_shape = (image_size, image_size, 1)\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    elif input_tensor is None:\n",
        "        print('You have to provide input_tensor when testing.')\n",
        "        exit()\n",
        "\n",
        "    # Define the model architecture.\n",
        "    model = keras.Sequential([\n",
        "      keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "      keras.layers.Conv2D(filters=32, kernel_size=kernel_size, activation='relu', padding='valid'),\n",
        "      keras.layers.Conv2D(filters=32, kernel_size=kernel_size, activation='relu', padding='valid'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "      keras.layers.Conv2D(filters=64, kernel_size=kernel_size, activation='relu', padding='valid'),\n",
        "      keras.layers.Conv2D(filters=64, kernel_size=kernel_size, activation='relu', padding='valid'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(200, activation='relu'),\n",
        "      keras.layers.Dense(200, activation='relu'),\n",
        "      keras.layers.Dense(nb_classes, name='before_softmax'),\n",
        "      keras.layers.Activation('softmax')\n",
        "    ])\n",
        "\n",
        "\n",
        "    if train:\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        # compiling\n",
        "        model.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          epochs=nb_epoch,\n",
        "          validation_split=0.1,\n",
        "        )\n",
        "\n",
        "\n",
        "        # save model\n",
        "        model.save_weights('./Model4.weights.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "\n",
        "        # Train the digit classification model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        model.load_weights('./Model4.weights.h5')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model4(train=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2VkQZUUpqQT"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QDymTSpcx2Z"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G9SowTKLpuab"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "# from keras.datasets import mnist\n",
        "# from keras import backend as K\n",
        "# from keras.models import Model\n",
        "# import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# import keras\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import math\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def cumulative_neuron_coverage(model_layer_dict1, model_layer_dict2, model_layer_dict3):\n",
        "    for (layer_name, index), v in model_layer_dict1.items():\n",
        "        model_layer_dict3[(layer_name, index)] = v or model_layer_dict2[(layer_name, index)]\n",
        "\n",
        "\n",
        "def neurons_covered_uncommon(model_layer_dict1, model_layer_dict2):\n",
        "    result = []\n",
        "    #dict1 are valid tests and dict2 are invalid\n",
        "    for (layer_name, index), v in model_layer_dict1.items():\n",
        "        if (not v) and model_layer_dict2[(layer_name, index)]:\n",
        "            result.append((layer_name, index))\n",
        "    return result\n",
        "\n",
        "def neuron_not_covered(model_layer_dict1):\n",
        "    result = []\n",
        "    for (layer_name, index), v in model_layer_dict1.items():\n",
        "        if (not v):\n",
        "            result.append((layer_name, index))\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "def delete_files_from_dir(dirPath, ext):\n",
        "    # eg input = /tmp/*.txt\n",
        "    fileFormat = dirPath + '*.' + ext\n",
        "    files = glob.glob(fileFormat)\n",
        "    for f in files:\n",
        "        try:\n",
        "            os.remove(f)\n",
        "        except OSError as e:\n",
        "            print(\"Error: %s : %s\" % (f, e.strerror))\n",
        "\n",
        "\n",
        "# This api is for sampling from latent space of VAE\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "# Logic for calculating reconstruction probability\n",
        "def reconstruction_probability(decoder, z_mean, z_log_var, X):\n",
        "    \"\"\"\n",
        "    :param decoder: decoder model\n",
        "    :param z_mean: encoder predicted mean value\n",
        "    :param z_log_var: encoder predicted sigma square value\n",
        "    :param X: input data\n",
        "    :return: reconstruction probability of input\n",
        "            calculated over L samples from z_mean and z_log_var distribution\n",
        "    \"\"\"\n",
        "    reconstructed_prob = np.zeros((X.shape[0],), dtype='float32')\n",
        "    L = 100\n",
        "    for l in range(L):\n",
        "        # print(\"[DEBUG] l = \", l)\n",
        "        sampled_zs = sampling([z_mean, z_log_var])\n",
        "        mu_hat, log_sigma_hat = decoder.predict(sampled_zs, steps=1)\n",
        "        log_sigma_hat = np.float64(log_sigma_hat)\n",
        "        sigma_hat = np.exp(log_sigma_hat) + 0.00001\n",
        "\n",
        "        loss_a = np.log(2 * np.pi * sigma_hat)\n",
        "        loss_m = np.square(mu_hat - X) / sigma_hat\n",
        "        reconstructed_prob += -0.5 * np.sum(loss_a + loss_m, axis=1)\n",
        "    reconstructed_prob /= L\n",
        "    return reconstructed_prob\n",
        "\n",
        "\n",
        "# Calculates and returns probability density of test input\n",
        "def calculate_density(x_target_orig, vae):\n",
        "    # print(\"Flag 7\")\n",
        "\n",
        "    x_target_orig = np.clip(x_target_orig, 0, 1)\n",
        "    x_target_orig = np.reshape(x_target_orig, (-1, 28*28))\n",
        "    x_target = np.reshape(x_target_orig, (-1, 28, 28, 1))\n",
        "    # print(\"Flag 8\")\n",
        "\n",
        "    z_mean, z_log_var, _ = vae.get_layer('encoder').predict(x_target,\n",
        "                                                            batch_size=128)\n",
        "    # print(\"Flag 9\")\n",
        "\n",
        "    reconstructed_prob_x_target = reconstruction_probability(vae.get_layer('decoder'), z_mean, z_log_var, x_target_orig)\n",
        "    # print(\"Flag 10\")\n",
        "\n",
        "    return reconstructed_prob_x_target\n",
        "\n",
        "\n",
        "# checks whether a test input is valid or invalid\n",
        "#Returns true if invalid\n",
        "def isInvalid(gen_img, vae, vae_threshold):\n",
        "    # print(\"Flag 5\")\n",
        "    gen_img_density = calculate_density(gen_img, vae)\n",
        "    # print(\"Flag 6\")\n",
        "\n",
        "    if gen_img_density < vae_threshold or math.isnan(gen_img_density):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMdxbgK7pyFv"
      },
      "source": [
        "# VAE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JnJSoAp8pzfd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code is implemented over the baseline provided in keras git repo\n",
        "https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n",
        "\n",
        "Example of VAE on MNIST dataset using MLP\n",
        "The VAE has a modular design. The encoder, decoder and VAE\n",
        "are 3 models that share weights. After training the VAE model,\n",
        "the encoder can be used to generate latent vectors.\n",
        "The decoder can be used to generate MNIST digits by sampling the\n",
        "latent vector from a Gaussian distribution with mean = 0 and std = 1.\n",
        "# Reference\n",
        "[1] Kingma, Diederik P., and Max Welling.\n",
        "\"Auto-Encoding Variational Bayes.\"\n",
        "https://arxiv.org/abs/1312.6114\n",
        "'''\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Lambda, Input, Dense, Reshape\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras import backend as K\n",
        "# from keras.optimizers import Adam\n",
        "from keras.models import model_from_json\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "# from utils import *\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "def Vae_MNIST_NN1(input_tensor=None, train=False):\n",
        "    np.random.seed(0)\n",
        "    # MNIST dataset\n",
        "    image_size = 28\n",
        "    if train:\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "        x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "        x_train = x_train.astype('float32') / 255\n",
        "        x_test = x_test.astype('float32') / 255\n",
        "\n",
        "        # network parameters\n",
        "        input_shape = (image_size, image_size, 1)\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "\n",
        "\n",
        "        # mnist = tf.keras.datasets.mnist\n",
        "        # (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "        # # Normalize the input image so that each pixel value is between 0 to 1.\n",
        "        # train_images = train_images / 255.0\n",
        "        # test_images = test_images / 255.0\n",
        "        # input_tensor = test_images\n",
        "\n",
        "\n",
        "        batch_size = 128\n",
        "        epochs = 1\n",
        "\n",
        "    elif input_tensor is None:\n",
        "        print('you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    latent_dim = 200\n",
        "    intermediate_dims = np.array([400])\n",
        "\n",
        "    # VAE model = encoder + decoder\n",
        "    # build encoder model\n",
        "    original_dim = image_size * image_size\n",
        "    inputs = Reshape((original_dim,), name='encoder_input')(input_tensor)\n",
        "    x = Dense(intermediate_dims[0], activation='relu')(inputs)\n",
        "    for i in range(intermediate_dims.shape[0]):\n",
        "        if i != 0:\n",
        "            x = Dense(intermediate_dims[i], activation='relu')(x)\n",
        "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "    # use reparameterization trick to push the sampling out as input\n",
        "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "    # instantiate encoder model\n",
        "    encoder = Model(input_tensor, [z_mean, z_log_var, z], name='encoder')\n",
        "    #encoder.summary()\n",
        "\n",
        "    # build decoder model\n",
        "    intermediate_dims = np.flipud(intermediate_dims)\n",
        "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "    x = Dense(intermediate_dims[0], activation='relu')(latent_inputs)\n",
        "    for i in range(intermediate_dims.shape[0]):\n",
        "        if i != 0:\n",
        "            x = Dense(intermediate_dims[i], activation='relu')(x)\n",
        "    pos_mean = Dense(original_dim, name='pos_mean')(x)\n",
        "    pos_log_var = Dense(original_dim, name='pos_log_var')(x)\n",
        "\n",
        "    # instantiate decoder model\n",
        "    decoder = Model(latent_inputs, [pos_mean, pos_log_var], name='decoder')\n",
        "    #decoder.summary()\n",
        "\n",
        "    # instantiate VAE model\n",
        "    outputs = decoder(encoder(input_tensor)[2])\n",
        "    vae = Model(input_tensor, outputs, name='vae_mlp')\n",
        "    #vae.summary()\n",
        "    if train:\n",
        "        # VAE loss = reconstruction_loss + kl_loss\n",
        "        loss_a = float(np.log(2 * np.pi)) + outputs[1]\n",
        "        loss_m = K.square(outputs[0] - inputs) / K.exp(outputs[1])\n",
        "        reconstruction_loss = -0.5 * K.sum((loss_a + loss_m), axis=-1)\n",
        "\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        vae_loss = K.mean(-reconstruction_loss + kl_loss)\n",
        "        vae.add_loss(vae_loss)\n",
        "        vae.compile(optimizer=\"adam\")\n",
        "        # vae.compile()\n",
        "        # vae.summary()\n",
        "        # vae.add_metric(reconstruction_loss, \"reconstruct\")\n",
        "        # vae.add_metric(kl_loss, \"kl\")\n",
        "        vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))\n",
        "        # save model\n",
        "        vae.save_weights('./vae_mnist_nn1.h5')\n",
        "    else:\n",
        "        # VAE loss = reconstruction_loss + kl_loss\n",
        "        loss_a = float(np.log(2 * np.pi)) + outputs[1]\n",
        "        loss_m = K.square(outputs[0] - inputs) / K.exp(outputs[1])\n",
        "        reconstruction_loss = -0.5 * K.sum((loss_a + loss_m), axis=-1)\n",
        "\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        vae_loss = K.mean(-reconstruction_loss + kl_loss)\n",
        "        vae.add_loss(vae_loss)\n",
        "\n",
        "        vae.compile(optimizer=\"adam\")\n",
        "\n",
        "\n",
        "        vae.load_weights('./vae_mnist_nn1.h5')\n",
        "\n",
        "    return vae\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Vae_MNIST_NN1(train=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtfNoYaZp3HA"
      },
      "source": [
        "# Test-Case Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rLju1fQ1_Lgp"
      },
      "outputs": [],
      "source": [
        "# !python3 -m pip uninstall -y tensorflow-model-optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HODVZ-1j1u5y"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dt-6zDeoiCRq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Code is built on top of DeepXplore code base.\n",
        "Density objective and VAE validation is added to the original objective function.\n",
        "We use DeepXplore as a baseline technique for test generation.\n",
        "\n",
        "DeepXplore: https://github.com/peikexin9/deepxplore\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "\n",
        "# import tf_keras as keras\n",
        "# from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "# from tf_keras.datasets import mnist\n",
        "# from tf_keras.layers import Input\n",
        "# from tf_keras.utils import to_categorical\n",
        "\n",
        "\n",
        "import imageio\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import datetime\n",
        "random.seed(3)\n",
        "\n",
        "# from __future__ import print_function\n",
        "\n",
        "# import argparse\n",
        "\n",
        "# from keras.datasets import mnist\n",
        "# from keras.layers import Input\n",
        "# # from keras.utils import to_categorical\n",
        "# # from Model1 import Model1\n",
        "# # from Model2 import Model2\n",
        "# # from Model3 import Model3\n",
        "# # from Model4 import Model4\n",
        "# # from Vae_MNIST_NN1 import Vae_MNIST_NN1\n",
        "# # from utils import *\n",
        "# import imageio\n",
        "# import numpy as np\n",
        "# import math\n",
        "# import time\n",
        "# import datetime\n",
        "# import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "\n",
        "# random.seed(3)\n",
        "\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "# parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "# parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "# parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "# parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "# parser.add_argument('weight_vae', help=\"weight hyperparm to control vae goal\", type=float)\n",
        "# parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "# parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "# parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "# parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "# parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "#                     choices=[0, 1, 2, 3], default=0, type=int)\n",
        "# parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "# parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# python3 dist_gen_diff.py occl 3 .5 .1 .1 50 20 .25 --target_model=$model;\n",
        "\n",
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.transformation = 'occl'\n",
        "    self.weight_diff = 3\n",
        "    self.weight_nc = 0.5\n",
        "    self.weight_vae = .1\n",
        "    self.step = .1\n",
        "    self.seeds = 50\n",
        "    self.grad_iterations = 20\n",
        "    self.threshold = .25\n",
        "    self.target_model = 0\n",
        "\n",
        "\n",
        "args = Args()\n",
        "\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "if args.weight_vae == 0:\n",
        "    output_directory = './baseline_generated_inputs_Model' + str(args.target_model + 1)+'/'+(args.transformation)+'/'\n",
        "else:\n",
        "    output_directory = './generated_inputs_Model' + str(args.target_model + 1)+'/'+(args.transformation)+'/'\n",
        "\n",
        "#Create directory to store generated tests\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "delete_files_from_dir(output_directory, 'png')\n",
        "\n",
        "# Create a subdirectory inside output directory\n",
        "# for saving original seed images used for test generation\n",
        "orig_directory = output_directory+'seeds/'\n",
        "if not os.path.exists(orig_directory):\n",
        "    os.makedirs(orig_directory)\n",
        "delete_files_from_dir(orig_directory, 'png')\n",
        "\n",
        "# VAE density threshold for classifying invalid inputs\n",
        "vae_threshold = -2708.34\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "img_dim = img_rows * img_cols\n",
        "# the data, shuffled and split between train and test sets\n",
        "(_, _), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "\n",
        "# define input tensor as a placeholder\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "# load multiple models sharing same input tensor\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "qmodel1 = QModel1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "if args.target_model == 3:\n",
        "    model3 = Model4(input_tensor=input_tensor)\n",
        "else:\n",
        "    model3 = Model3(input_tensor=input_tensor)\n",
        "vae = Vae_MNIST_NN1(input_tensor=input_tensor)\n",
        "\n",
        "# init coverage table\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)\n",
        "\n",
        "if args.weight_vae == 0:\n",
        "    print(\"*****Running baseline test....\")\n",
        "else:\n",
        "    print(\"*****Running VAE+Baseline test....\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJgpzwL1zKj"
      },
      "source": [
        "## Input Genenaration Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKJWJc9Wp6Hw",
        "outputId": "e56db519-8536-45a2-a21f-55523701c512"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] loop_index =  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-8ec13f51c329>:233: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  if gen_img_density < vae_threshold or math.isnan(gen_img_density):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  2\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  3\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  4\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  5\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  6\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  7\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  8\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  9\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  10\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  11\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  12\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  13\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  14\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  15\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  16\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  17\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  18\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  19\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  20\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  21\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  22\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  23\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  24\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  25\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  26\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  27\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  28\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  29\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  30\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  31\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  32\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  33\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  34\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  35\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  36\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  37\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  38\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  39\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  40\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  41\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  42\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  43\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  44\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  45\n",
            "input already causes different outputs: 0, 7, 7\n",
            "[DEBUG] loop_index =  46\n",
            "input already causes different outputs: 0, 4, 7\n",
            "[DEBUG] loop_index =  47\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==============================================================================================\n",
        "# start gen inputs\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "seed_nums = np.load('./seeds/seeds_'+str(args.seeds)+'.npy')\n",
        "result_loop_index = []\n",
        "result_coverage = []\n",
        "loop_index = 0\n",
        "for current_seed in seed_nums:\n",
        "    # below logic is to track number of iterations under progress\n",
        "    loop_index += 1\n",
        "    print(\"[DEBUG] loop_index = \", loop_index)\n",
        "    gen_img = np.expand_dims(x_test[current_seed], axis=0)\n",
        "    orig_img = gen_img.copy()\n",
        "    # first check if input already induces differences\n",
        "    label1, label2, label3 = np.argmax(qmodel1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(\n",
        "        model3.predict(gen_img)[0])\n",
        "\n",
        "    if not label1 == label2 == label3 and not isInvalid(gen_img, vae, vae_threshold):\n",
        "        print('input already causes different outputs: {}, {}, {}'.format(label1, label2, label3))\n",
        "\n",
        "        update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "        update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "        update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "        if args.target_model == 0:\n",
        "            result_coverage.append(neuron_covered(model_layer_dict1)[2])\n",
        "        elif args.target_model == 1:\n",
        "            result_coverage.append(neuron_covered(model_layer_dict2)[2])\n",
        "        elif args.target_model == 2:\n",
        "            result_coverage.append(neuron_covered(model_layer_dict3)[2])\n",
        "        elif args.target_model == 3:\n",
        "            result_coverage.append(neuron_covered(model_layer_dict3)[2])\n",
        "        #print('covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "        #      % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "        #         neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "        #         neuron_covered(model_layer_dict3)[2]))\n",
        "        #averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "        #               neuron_covered(model_layer_dict3)[0]) / float(\n",
        "        #    neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "        #    neuron_covered(model_layer_dict3)[\n",
        "        #        1])\n",
        "        #print('averaged covered neurons %.3f' % averaged_nc)\n",
        "\n",
        "        gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "        # save the result to disk\n",
        "        imageio.imwrite(output_directory + 'already_differ_' + str(current_seed) + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png', gen_img_deprocessed)\n",
        "        continue\n",
        "\n",
        "\n",
        "    # if all label agrees\n",
        "    orig_label = label1\n",
        "    layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "    layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "    layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "    # construct joint loss function\n",
        "    if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('quant_before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('quant_before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('quant_before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 3:\n",
        "        loss1 = K.mean(model1.get_layer('quant_before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "    loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "    loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "\n",
        "    # vae reconstruction probability\n",
        "    vae_input = vae.get_layer('encoder').get_layer('encoder_input').output\n",
        "    vae_output = vae.outputs\n",
        "    loss_a = float(np.log(2 * np.pi)) + vae_output[1]\n",
        "    loss_m = K.square(vae_output[0] - vae_input) / K.exp(vae_output[1])\n",
        "    vae_reconstruction_prob = -0.5 * K.sum((loss_a + loss_m), axis=-1)\n",
        "    vae_reconstruction_prob = vae_reconstruction_prob/img_dim\n",
        "\n",
        "    layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron) + args.weight_vae * vae_reconstruction_prob\n",
        "\n",
        "    # for adversarial image generation\n",
        "    final_loss = K.mean(layer_output)\n",
        "\n",
        "    # we compute the gradient of the input picture wrt this loss\n",
        "    grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "    # this function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "    # Running gradient ascent\n",
        "    for iters in range(args.grad_iterations):\n",
        "        # gen_img = np.reshape(gen_img, (28,28,1))\n",
        "        # gen_img = gen_img.reshape(-1,28,28,1)\n",
        "        # gen_img = gen_img[None, ...]\n",
        "        print(gen_img.shape)\n",
        "        X = [gen_img]\n",
        "        temp = iterate(X)\n",
        "\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = temp\n",
        "\n",
        "        #Apply domain specific constraints\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)\n",
        "\n",
        "        # generate the new test input\n",
        "        gen_img += grads_value * args.step\n",
        "        gen_img = np.clip(gen_img, 0, 1)\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            if isInvalid(gen_img, vae, vae_threshold):\n",
        "                # print(\"generated outlier, not saving \",loop_index, iters)\n",
        "                continue\n",
        "\n",
        "            #print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "            #print(\"generated valid input at loop index for current_seed\", loop_index, current_seed)\n",
        "\n",
        "            # Update coverage\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "            #print('covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "            #      % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "            #         neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "            #         neuron_covered(model_layer_dict3)[2]))\n",
        "            #averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "            #               neuron_covered(model_layer_dict3)[0]) / float(\n",
        "            #    neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "            #    neuron_covered(model_layer_dict3)[\n",
        "            #        1])\n",
        "            #print('averaged covered neurons %.3f' % averaged_nc)\n",
        "\n",
        "            # Track the seed numbers and coverage achieved for final result\n",
        "            result_loop_index.append(loop_index)\n",
        "            if args.target_model == 0:\n",
        "                result_coverage.append(neuron_covered(model_layer_dict1)[2])\n",
        "            elif args.target_model == 1:\n",
        "                result_coverage.append(neuron_covered(model_layer_dict2)[2])\n",
        "            elif args.target_model == 2:\n",
        "                result_coverage.append(neuron_covered(model_layer_dict3)[2])\n",
        "            elif args.target_model == 3:\n",
        "                result_coverage.append(neuron_covered(model_layer_dict3)[2])\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            imageio.imwrite(\n",
        "                    output_directory + str(loop_index) + '_' + str(\n",
        "                        predictions1) + '_' + str(predictions2) + '_' + str(predictions3)+'.png',\n",
        "                    gen_img_deprocessed)\n",
        "            imageio.imwrite(\n",
        "                    orig_directory + str(loop_index) + '_' + str(\n",
        "                        predictions1) + '_' + str(predictions2) + '_' + str(predictions3)+'_orig.png',\n",
        "                    orig_img_deprocessed)\n",
        "            break\n",
        "\n",
        "duration = (datetime.datetime.now() - start_time).total_seconds()\n",
        "no_tests = len(result_loop_index)\n",
        "\n",
        "if args.weight_vae == 0:\n",
        "    print(\"**** Result of baseline test:\")\n",
        "else:\n",
        "    print(\"**** Result of VAE+Baseline test:\")\n",
        "\n",
        "print(\"No of test inputs generated: \", no_tests)\n",
        "if no_tests == 0:\n",
        "    print(\"Cumulative coverage for tests: 0\")\n",
        "    print('Avg. test generation time: NA s')\n",
        "else:\n",
        "    print(\"Cumulative coverage for tests: \", round(result_coverage[-1],3))\n",
        "    print('Avg. test generation time: {} s'.format(round(duration/no_tests),2))\n",
        "print('Total time: {} s'.format(round(duration, 2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im40SnKibzLE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting\n",
        "data = result_coverage\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data, marker='o')\n",
        "plt.xlabel('Time (Tick)')\n",
        "plt.ylabel('Coverage(%)')\n",
        "plt.title('Plot of the Provided Data List')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddmKXsfjTVCL"
      },
      "outputs": [],
      "source": [
        "!tar -czvf occl.tar.gz /content/generated_inputs_Model1/occl"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fMdxbgK7pyFv"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}